{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from typing import Annotated, List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "import uuid\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.path.join('../config/','.env'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        \"\"\"\n",
    "        Initialize the Assistant with a runnable object.\n",
    "\n",
    "        Args:\n",
    "            runnable (Runnable): The runnable instance to invoke.\n",
    "        \"\"\"\n",
    "        self.runnable = runnable\n",
    "\n",
    "    def __call__(self, state: State, config: RunnableConfig):\n",
    "        \"\"\"\n",
    "        Call method to invoke the LLM and handle its responses.\n",
    "        Re-prompt the assistant if the response is not a tool call or meaningful text.\n",
    "\n",
    "        Args:\n",
    "            state (State): The current state containing messages.\n",
    "            config (RunnableConfig): The configuration for the runnable.\n",
    "\n",
    "        Returns:\n",
    "            dict: The final state containing the updated messages.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            result = self.runnable.invoke(state)  # Invoke the LLM\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                break\n",
    "        return {\"messages\": result}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_tool_error(state: State) -> dict:\n",
    "    error = state.get(\"error\")\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "def conversation_state_tracker(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the conversation history to determine the current state of the conversation.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): Pass the query and find the current state of the conversation.\n",
    "    \n",
    "    Returns:\n",
    "    - One from the following (str):\n",
    "        - Initial -> meaning that the user is at The beginning stage of the conversation.\n",
    "        - Exploring -> meaning that the user is at an In-depth discussion and exploration of topics.\n",
    "        - Probing -> meaning that the user is Asking deeper questions to uncover more information.\n",
    "        - Concluding -> meaning that the user is at Wrapping up the conversation and reaching a conclusion.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        Analyze the following conversation history:\n",
    "        {query}\n",
    "        Determine the current conversation state in Socratic learning Method to decide what to do next. \n",
    "        Consider factors such as the topic, depth of discussion, and user engagement. \n",
    "        Respond with only on of the possible states:\n",
    "            - Initial -> meaning that the user is at The beginning stage of the conversation.\n",
    "            - Exploring -> meaning that the user is at an In-depth discussion and exploration of topics.\n",
    "            - Probing -> meaning that the user is Asking deeper questions to uncover more information.\n",
    "            - Concluding -> meaning that the user is at Wrapping up the conversation and reaching a conclusion.\n",
    "        \n",
    "        Return one from this List [Initial,Exploring,Probing,Concluding]\n",
    "        \n",
    "        Example:\n",
    "            -   query: \"Hi, I'm new to Machine Learning, where should I start?\" \n",
    "                return: 'Initial'\n",
    "            -   query: \"Can you explain the difference between supervised and unsupervised learning?\" \n",
    "                return: 'Exploring'\n",
    "            -   query: \"What happens if we use a high learning rate in training?\" \n",
    "                return: 'Probing'\n",
    "            -   query: \"Got it, thanks for your help with Machine Learning basics.\" \n",
    "                return: 'Concluding'\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"llama3-groq-tool-use\",\n",
    "    # model=\"llama3.1\",\n",
    "    temperature=0,\n",
    ")\n",
    "tools = [conversation_state_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the primary assistant prompt template\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant you're purpose is to return the current state of the conversation.\"\n",
    "            \"You have access to one tool: conversation_state_tracker.\"\n",
    "            \"You must use this tool whenever you're being queried and respond with the current state, don't add anything else to your response.\"\n",
    "            \"You can use this tool to get the current state of the conversation by sending the user input as query to the tool.\"\n",
    "            \"Strictly return the tool's output as your response.\"\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Prompt our LLM and bind tools\n",
    "assistant_runnable = primary_assistant_prompt | llm.bind_tools(tools, tool_choice='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "memory = MemorySaver()\n",
    "react_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# Show\n",
    "# display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_react_agent_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "    messages= react_graph.invoke({\"messages\": (\"user\", example[\"input\"])}, config)\n",
    "    return {\"response\": messages[\"messages\"][-1].content, \"messages\": messages}\n",
    "\n",
    "\n",
    "example = {\"input\": \"Hi, I wanna learn Datastructures\"}\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "messages= react_graph.invoke({\"messages\": (\"user\", example[\"input\"])}, config)\n",
    "# print({\"response\": messages[\"messages\"][-1].content, \"messages\": messages})\n",
    "# response = predict_react_agent_answer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi, I wanna learn Datastructures', id='1ee2f3b2-aa76-43ea-8e3f-ca49eabe3649'),\n",
       "  AIMessage(content='Sure! What specific aspect of data structures are you interested in? For example, arrays, linked lists, stacks, queues, trees, graphs, etc.', response_metadata={'model': 'llama3-groq-tool-use', 'created_at': '2024-09-01T16:04:23.405101035Z', 'message': {'role': 'assistant', 'content': 'Sure! What specific aspect of data structures are you interested in? For example, arrays, linked lists, stacks, queues, trees, graphs, etc.'}, 'done_reason': 'stop', 'done': True, 'total_duration': 11721649583, 'load_duration': 3822687895, 'prompt_eval_count': 262, 'prompt_eval_duration': 3717044000, 'eval_count': 32, 'eval_duration': 4096965000}, id='run-37541952-ce9a-400d-9e17-b4b23e040016-0', usage_metadata={'input_tokens': 262, 'output_tokens': 32, 'total_tokens': 294})]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
